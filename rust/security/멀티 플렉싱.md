
# Multi Flexing

- 하나의 연결안데 여러개의 독립적인 Stream이 있는 상태를 말한다

- UDP 자체에는 없는 개념이고 어플리케이션 레벨 에서 직접 구현되는 부분이다

### QUIC

- **UDP 위에 연결 1개를 만든다**
- 그 **연결 안에 스트림을 여러 개 만든다**
```scss
[ QUIC Connection ]
   ├─ Stream 1 (파일 다운로드)
   ├─ Stream 2 (제어 메시지)
   ├─ Stream 3 (터널 데이터)
```
- 각 스트림은 **논리적으로 독립**
- Stream 1이 잠시 막혀도 → Stream 2, 3은 계속 흐른다
    

👉 이게 **멀티플렉싱**이다.


지금 네가 만드는 보안 터널에서:

- Stream A: ping / 제어 메시지
    
- Stream B: SSH 포워딩
    
- Stream C: HTTP 프록시
    

을 **전부 하나의 QUIC 연결**로 처리할 수 있다.

즉:

- 인증(mTLS)은 **한 번만**
    
- 연결도 **하나만**
    
- 내부에서는 **여러 작업을 동시에** 수행



# 구현 원리

## 식별자 
- 핵심은 식별자(ID) 이다.
```scss
[ UDP Packet ]
┌───────────────┬───────────────┐
│ Channel ID    │ Payload       │
│ (Stream ID)   │ (Data)        │
└───────────────┴───────────────┘

```

- Header의 ID를 읽어서 어떤 데이터 map으로 들어가는지 분기한다 
```
recvfrom()
 → parse header
 → channel_id 확인
 → 해당 채널 처리 로직으로 전달

```



# 구현 방법

## 1. 포트 기반-OS 레벨 제어

- 단순 한 방법
- 서로 다른 UDP 포트를 사용
- 애초에 보내는 쪽부터 다른 경로를 사용

### 단점 
- 포트소모
- NAT/방화벽에 약함

## 2. 헤더 기반 - 애플리케이션 레벨, 가장 일반적

- 같은 포트 사용
- 패킷 내부에 ID값 포함 
	- 각 ID에 해당하는 hashmap에 데이터 저장

- 포트 1개로 무한 채널
- NAT 친화적 
- QUIC, WebRTC, WireGuard 전부 이 방식


### 자료 구조 
### A. `HashMap<ChannelId, ChannelState>` 가 기본

- `ChannelId`: `u32` 같은 정수 ID
- `ChannelState`: 그 채널에 필요한 모든 상태

예시(개념):
```rust
type ChannelId = u32;

struct ChannelState {
    rx_queue: VecDeque<Vec<u8>>,   // 순서대로 받은 payload를 쌓아둠(단순형)
    // 또는 재조립/신뢰성/흐름제어를 넣으면 아래 같은 것들이 추가됨
    reorder: BTreeMap<u64, Vec<u8>>, // seq 기반 재정렬 버퍼
    next_seq: u64,                   // 다음에 처리할 시퀀스
    last_seen: Instant,              // 타임아웃/GC용
}
```


---

### Level 1: “메시지 단위” 재조립(분할 전송)

UDP는 MTU 제한이 있어서 큰 데이터를 쪼개야 한다.

패킷 헤더에 보통:

- `channel_id`
- `msg_id`
- `frag_idx`
- `frag_count`

자료구조:

- `HashMap<(channel_id, msg_id), ReassemblyBuffer>`
    
```rust
struct ReassemblyBuffer {     
	total_frags: u16,
    got: Vec<Option<Vec<u8>>>, // 조각별로 저장 
}

```

- 모든 조각이 모이면 합쳐서 `ChannelState.rx_queue`로 넘김


### Level 2: “스트림”처럼 만들기(순서/재전송/흐름제어)

이게 QUIC이 하는 일이다. 너도 구현할 수는 있지만 복잡해진다.

패킷 헤더에:

- `channel_id`
    
- `seq` (순서 번호)
    
- `ack` / `ack_ranges`
    
- `window` (흐름제어)
    

자료구조(전형):

- 수신: `BTreeMap<seq, payload>`로 재정렬 버퍼
    
- 송신: `HashMap<seq, SentPacket>`로 미확인 패킷 보관(재전송용)
```rust
struct ChannelState {
    recv_next: u64,
    recv_buf: BTreeMap<u64, Vec<u8>>, // seq→payload
    send_next: u64,
    inflight: HashMap<u64, Vec<u8>>,  // 재전송 후보
}
```


## 처리 루틴 구조(보통 이렇게 짠다)

### 핵심: “수신 루프”와 “채널 핸들러” 분리

- 수신 루프: `recvfrom()`로 패킷 받고 → 헤더 파싱 → 해당 채널 상태 업데이트
    
- 채널 핸들러: 큐에서 메시지 꺼내 처리
    

Rust에서 흔한 패턴 2개:

#### 패턴 1) 단일 스레드 이벤트 루프 + HashMap

- 성능 좋고 단순
    
- 모든 상태를 한 스레드에서 관리
    

#### 패턴 2) 채널별 `mpsc` 큐 (Tokio)

- `HashMap<id, mpsc::Sender<Msg>>`
    
- 새 채널 생기면 task 생성하고 sender 저장
    
- 패킷 오면 sender로 넘김